{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from models.configuration_alignable_model import AlignableRepresentationConfig, AlignableConfig\n",
    "from models.alignable_base import AlignableModel\n",
    "from models.interventions import BoundlessRotatedSpaceIntervention\n",
    "from models.llama.modelings_alignable_llama import create_llama\n",
    "from models.basic_utils import set_seed, count_parameters\n",
    "\n",
    "from utils.prompt_utils import *\n",
    "from utils.intervention_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.extract_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_icl_examples = 10\n",
    "N_TRIALS = 100\n",
    "\n",
    "prefixes = {\"input\":\"Q:\", \"output\":\"A:\", \"instructions\":\"\"}\n",
    "separators = {\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f3ff121a4747b7a5b49633e0eaa6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/data/public_models/llama/llama_hf_weights/llama-7b/\")\n",
    "llama = LlamaForCausalLM.from_pretrained(\"/data/public_models/llama/llama_hf_weights/llama-7b/\")\n",
    "\n",
    "_ = llama.to(\"cuda\") # single gpu\n",
    "_ = llama.eval()         # always no grad on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"antonym\", root_data_dir=\"../dataset_files\", test_size=0.3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if prefixes is not None and separators is not None:\n",
    "    dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, prefixes=prefixes, separators=separators)\n",
    "else:\n",
    "    dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer)\n",
    "    \n",
    "filter_set = filter_set = np.arange(len(dataset['valid']))\n",
    "\n",
    "\n",
    "torch_dataset = []\n",
    "\n",
    "for n in range(N_TRIALS):\n",
    "    \n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "    word_pairs_test = dataset['valid'][np.random.choice(filter_set, 1, replace=False)]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, \n",
    "                                                        shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "    _, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    \n",
    "    data_pair = preprocess([prompt_string], [target], tokenizer)        \n",
    "    torch_dataset.append(data_pair)\n",
    "    \n",
    "torch_dataset = Dataset.from_list(torch_dataset)\n",
    "torch_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(tqdm(torch_dataset)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama.device)\n",
    "        \n",
    "        # aligning forward!\n",
    "        outputs = llama(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            labels=inputs['labels'],\n",
    "        )\n",
    "        \n",
    "        actual_test_labels = inputs['labels'][:, -1]\n",
    "        pred_test_labels = outputs.logits[:, -2].argmax(dim=-1)    \n",
    "\n",
    "        correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "        total_count += len(correct_labels)\n",
    "        correct_count += correct_labels.sum().tolist()\n",
    "current_acc = round(correct_count/total_count, 2)\n",
    "print(f\"[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trainset and valset for Boundless DAS\n",
    "\n",
    "das_train_set = []\n",
    "das_eval_set = []\n",
    "\n",
    "for n in range(N_TRIALS):\n",
    "    \n",
    "    noninformative_word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "\n",
    "    word_pairs_test = dataset['valid'][np.random.choice(filter_set, 1, replace=False)]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "    noninformative_prompt_data = word_pairs_to_prompt_data(noninformative_word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=True, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "\n",
    "    source_token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    token_labels, noninformative_prompt_string = get_token_meta_labels(noninformative_prompt_data, tokenizer, query)\n",
    "\n",
    "    data_pair = preprocess([noninformative_prompt_string], [target], tokenizer)\n",
    "    data_pair[\"source_input_ids\"] = preprocess([prompt_string], [target], tokenizer)[\"input_ids\"]\n",
    "    \n",
    "    assert source_token_labels[-1][2] == \"query_predictive_token\"\n",
    "    source_predictive_token_idxs = source_token_labels[-1][0]\n",
    "    data_pair[\"source_predictive_token_idxs\"] = source_predictive_token_idxs\n",
    "    \n",
    "    assert token_labels[-1][2] == \"query_predictive_token\"\n",
    "    predictive_token_idxs = token_labels[-1][0]\n",
    "    data_pair[\"predictive_token_idxs\"] = predictive_token_idxs\n",
    "    \n",
    "    das_train_set.append(data_pair)\n",
    "\n",
    "for n in range(N_TRIALS):\n",
    "    noninformative_word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "\n",
    "    word_pairs_test = dataset['test'][np.random.choice(filter_set, 1, replace=False)]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "    noninformative_prompt_data = word_pairs_to_prompt_data(noninformative_word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=True, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "\n",
    "    source_token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    token_labels, noninformative_prompt_string = get_token_meta_labels(noninformative_prompt_data, tokenizer, query)\n",
    "\n",
    "    data_pair = preprocess([noninformative_prompt_string], [target], tokenizer)\n",
    "    data_pair[\"source_input_ids\"] = preprocess([prompt_string], [target], tokenizer)[\"input_ids\"]\n",
    "    \n",
    "    assert source_token_labels[-1][2] == \"query_predictive_token\"\n",
    "    source_predictive_token_idxs = source_token_labels[-1][0]\n",
    "    data_pair[\"source_predictive_token_idxs\"] = source_predictive_token_idxs\n",
    "    \n",
    "    assert token_labels[-1][2] == \"query_predictive_token\"\n",
    "    predictive_token_idxs = token_labels[-1][0]\n",
    "    data_pair[\"predictive_token_idxs\"] = predictive_token_idxs\n",
    "    \n",
    "    das_eval_set.append(data_pair)\n",
    "    \n",
    "\n",
    "das_train_set = Dataset.from_list(das_train_set)\n",
    "das_eval_set = Dataset.from_list(das_eval_set)\n",
    "das_train_set.set_format(type='torch', columns=['input_ids', 'labels', 'source_input_ids', 'source_predictive_token_idxs', 'predictive_token_idxs'])\n",
    "das_eval_set.set_format(type='torch', columns=['input_ids', 'labels', 'source_input_ids', 'source_predictive_token_idxs', 'predictive_token_idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_boundless_das_position_config(model_type, intervention_type, layer):\n",
    "    alignable_config = AlignableConfig(\n",
    "        alignable_model_type=model_type,\n",
    "        alignable_representations=[\n",
    "            AlignableRepresentationConfig(\n",
    "                layer,             # layer\n",
    "                intervention_type, # intervention type\n",
    "                \"pos\",             # intervention unit\n",
    "                1                  # max number of unit\n",
    "            ),\n",
    "        ],\n",
    "        alignable_interventions_type=BoundlessRotatedSpaceIntervention,\n",
    "    )\n",
    "    return alignable_config\n",
    "\n",
    "alignable_config = simple_boundless_das_position_config(type(llama), \"block_output\", 15)\n",
    "alignable = AlignableModel(alignable_config, llama)\n",
    "alignable.set_device(\"cuda\")\n",
    "alignable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = int(len(das_train_set) * 3)\n",
    "warm_up_steps = 0.1 * t_total\n",
    "optimizer_params = []\n",
    "\n",
    "for k, v in alignable.interventions.items():\n",
    "    optimizer_params += [{'params': v[0].rotate_layer.parameters()}]\n",
    "    optimizer_params += [{'params': v[0].intervention_boundaries, 'lr': 1e-2}]\n",
    "    \n",
    "optimizer = torch.optim.Adam(\n",
    "    optimizer_params,\n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warm_up_steps,\n",
    "    num_training_steps=t_total\n",
    ")\n",
    "\n",
    "# You can define your custom compute_metrics function.\n",
    "def compute_metrics(eval_preds, eval_labels):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for eval_pred, eval_label in zip(eval_preds, eval_labels):\n",
    "        actual_test_labels = eval_label[:, -1]\n",
    "        pred_test_labels = torch.argmax(eval_pred[:, -2], dim=-1)\n",
    "        correct_labels = (actual_test_labels==pred_test_labels)\n",
    "        total_count += len(correct_labels)\n",
    "        correct_count += correct_labels.sum().tolist()\n",
    "    accuracy = round(correct_count/total_count, 2)\n",
    "    return {\"accuracy\" : accuracy}\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 1\n",
    "total_step = 0\n",
    "target_total_step = len(das_train_set) * epochs\n",
    "temperature_start = 50.0\n",
    "temperature_end = 0.1\n",
    "temperature_schedule = torch.linspace(\n",
    "    temperature_start, temperature_end, target_total_step\n",
    ").to(torch.bfloat16).to(\"cuda\")\n",
    "alignable.set_temperature(temperature_schedule[total_step])\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    shift_logits = logits[..., :, :].contiguous()\n",
    "    shift_labels = labels[..., :].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    shift_logits = shift_logits.view(-1, alignable.model_config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "    \n",
    "    for k, v in alignable.interventions.items():\n",
    "        boundary_loss = 1. * v[0].intervention_boundaries.sum()\n",
    "    loss += boundary_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  16777218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 100/100 [04:50<00:00,  2.91s/it, loss=5.37, acc=0]\n",
      "Epoch: 1: 100%|██████████| 100/100 [05:03<00:00,  3.03s/it, loss=3.6, acc=0]\n",
      "Epoch: 2: 100%|██████████| 100/100 [05:03<00:00,  3.04s/it, loss=1.88, acc=0]\n",
      "Epoch: 100%|██████████| 3/3 [14:57<00:00, 299.25s/it]\n"
     ]
    }
   ],
   "source": [
    "alignable.model.train() # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(alignable.model))\n",
    "print(\"intervention trainable parameters: \", alignable.count_parameters())\n",
    "train_iterator = trange(\n",
    "    0, int(epochs), desc=\"Epoch\"\n",
    ")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        das_train_set, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        _, counterfactual_outputs = alignable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "            {\"sources->base\": ([[[inputs[\"source_predictive_token_idxs\"]]]*b_s], [[[inputs[\"predictive_token_idxs\"]]]*b_s])}\n",
    "        )\n",
    "        eval_metrics = compute_metrics(\n",
    "            [counterfactual_outputs.logits], [inputs['labels']]\n",
    "        )\n",
    "        \n",
    "        # loss and backprop\n",
    "        loss = calculate_loss(\n",
    "            counterfactual_outputs.logits, inputs[\"labels\"]\n",
    "        )\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({'loss': loss_str, 'acc': eval_metrics[\"accuracy\"]})\n",
    "        \n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                alignable.set_zero_grad()\n",
    "                alignable.set_temperature(temperature_schedule[total_step])\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation on the test set\n",
    "eval_labels = []\n",
    "eval_preds = []\n",
    "with torch.no_grad():\n",
    "    epoch_iterator = tqdm(das_eval_set, desc=f\"Test\")\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        _, counterfactual_outputs = alignable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "            {\"sources->base\": ([[[inputs[\"source_predictive_token_idxs\"]]]*b_s], [[[inputs[\"predictive_token_idxs\"]]]*b_s])} # swap 80th token\n",
    "        )\n",
    "        eval_labels += [inputs['labels']]\n",
    "        eval_preds += [counterfactual_outputs.logits]\n",
    "eval_metrics = compute_metrics(eval_preds, eval_labels)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
