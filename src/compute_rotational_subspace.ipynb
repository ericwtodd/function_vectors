{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from models.configuration_alignable_model import AlignableRepresentationConfig, AlignableConfig\n",
    "from models.alignable_base import AlignableModel\n",
    "from models.interventions import BoundlessRotatedSpaceIntervention\n",
    "from models.llama.modelings_alignable_llama import create_llama\n",
    "from models.basic_utils import set_seed, count_parameters\n",
    "\n",
    "from utils.prompt_utils import *\n",
    "from utils.intervention_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.extract_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_icl_examples = 10\n",
    "N_TRIALS = 32\n",
    "\n",
    "prefixes = {\"input\":\"Q:\", \"output\":\"A:\", \"instructions\":\"\"}\n",
    "separators = {\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55fad88467d4dfd943f455303cc779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/work/frink/models/llama_7b/\")\n",
    "llama = LlamaForCausalLM.from_pretrained(\"/work/frink/models/llama_7b/\", torch_dtype=torch.float16)\n",
    "\n",
    "_ = llama.to(\"cuda\")        # single gpu\n",
    "_ = llama.eval()            # always no grad on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"antonym\", root_data_dir=\"../dataset_files\", test_size=0.3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \n",
    "    if len(batch[0].keys()) == 2:\n",
    "        \n",
    "        input_ids, labels = tuple([data_pair[key] for data_pair in batch] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "        )\n",
    "        \n",
    "    elif len(batch[0].keys()) == 5:\n",
    "        input_ids, labels, source_input_ids, source_predictive_token_idxs, predictive_token_idxs = tuple([data_pair[key] for data_pair in batch] for key in ('input_ids', 'labels', 'source_input_ids', 'source_predictive_token_idxs', 'predictive_token_idxs'))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        source_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            source_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        source_predictive_token_idxs = torch.LongTensor(source_predictive_token_idxs)\n",
    "        predictive_token_idxs = torch.LongTensor(predictive_token_idxs)\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "            source_input_ids=source_input_ids,\n",
    "            source_attention_mask=source_input_ids.ne(tokenizer.pad_token_id),\n",
    "            predictive_token_idxs=predictive_token_idxs,\n",
    "            source_predictive_token_idxs=source_predictive_token_idxs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if prefixes is not None and separators is not None:\n",
    "    dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, prefixes=prefixes, separators=separators)\n",
    "else:\n",
    "    dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer)\n",
    "    \n",
    "filter_set = filter_set = np.arange(len(dataset['valid']))\n",
    "\n",
    "\n",
    "torch_dataset = []\n",
    "\n",
    "for n in range(N_TRIALS):\n",
    "    \n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "    word_pairs_test = dataset['valid'][np.random.choice(filter_set, 1, replace=False)]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, \n",
    "                                                        shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "    _, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    \n",
    "    data_pair = preprocess([prompt_string], [target], tokenizer) \n",
    "    torch_dataset.append(data_pair)\n",
    "    \n",
    "torch_dataset = Dataset.from_list(torch_dataset)\n",
    "torch_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "\n",
    "dataloader = DataLoader(torch_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(tqdm(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama.device)\n",
    "                    \n",
    "        # aligning forward!\n",
    "        outputs = llama(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            labels=inputs['labels'],\n",
    "            attention_mask=inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "            label_idxs = inputs['labels'][i].ne(IGNORE_INDEX).nonzero().squeeze(-1)\n",
    "            # label_idxs = label_idxs[1: ]\n",
    "            left_shifted_idxs = label_idxs\n",
    "            \n",
    "            actual_test_labels = inputs['labels'][i][label_idxs].tolist()\n",
    "            pred_test_labels = [outputs.logits[i][idx].argmax(dim=-1) for idx in left_shifted_idxs]\n",
    "            \n",
    "            correct = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += 1\n",
    "            if correct:\n",
    "                correct_count += 1\n",
    "                \n",
    "current_acc = round(correct_count/total_count, 2)\n",
    "print(f\"[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trainset and valset for Boundless DAS\n",
    "\n",
    "das_train_set = []\n",
    "das_eval_set = []\n",
    "zs_das_eval_set = []\n",
    "\n",
    "for n in range(N_TRIALS):\n",
    "    \n",
    "    noninformative_word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "\n",
    "    word_pairs_test = dataset['valid'][np.random.choice(filter_set, 1, replace=False)]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "    noninformative_prompt_data = word_pairs_to_prompt_data(noninformative_word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=True, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "\n",
    "    source_token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    token_labels, noninformative_prompt_string = get_token_meta_labels(noninformative_prompt_data, tokenizer, query)\n",
    "\n",
    "    data_pair = preprocess([noninformative_prompt_string], [target], tokenizer)\n",
    "    data_pair[\"source_input_ids\"] = preprocess([prompt_string], [target], tokenizer)[\"input_ids\"]\n",
    "    \n",
    "    assert source_token_labels[-1][2] == \"query_predictive_token\"\n",
    "    source_predictive_token_idxs = source_token_labels[-1][0]\n",
    "    data_pair[\"source_predictive_token_idxs\"] = source_predictive_token_idxs\n",
    "    \n",
    "    assert token_labels[-1][2] == \"query_predictive_token\"\n",
    "    predictive_token_idxs = token_labels[-1][0]\n",
    "    data_pair[\"predictive_token_idxs\"] = predictive_token_idxs\n",
    "    \n",
    "    das_train_set.append(data_pair)\n",
    "\n",
    "for n in range(len(dataset[\"test\"])):\n",
    "    \n",
    "    noninformative_word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "\n",
    "    word_pairs_test = dataset['test'][n]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "    noninformative_prompt_data = word_pairs_to_prompt_data(noninformative_word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=True, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "\n",
    "    source_token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    token_labels, noninformative_prompt_string = get_token_meta_labels(noninformative_prompt_data, tokenizer, query)\n",
    "\n",
    "    data_pair = preprocess([noninformative_prompt_string], [target], tokenizer)\n",
    "    data_pair[\"source_input_ids\"] = preprocess([prompt_string], [target], tokenizer)[\"input_ids\"]\n",
    "    \n",
    "    assert source_token_labels[-1][2] == \"query_predictive_token\"\n",
    "    source_predictive_token_idxs = source_token_labels[-1][0]\n",
    "    data_pair[\"source_predictive_token_idxs\"] = source_predictive_token_idxs\n",
    "    \n",
    "    assert token_labels[-1][2] == \"query_predictive_token\"\n",
    "    predictive_token_idxs = token_labels[-1][0]\n",
    "    data_pair[\"predictive_token_idxs\"] = predictive_token_idxs\n",
    "    \n",
    "    das_eval_set.append(data_pair)\n",
    "    \n",
    "\n",
    "for n in range(len(dataset[\"test\"])):\n",
    "    \n",
    "    zs_word_pairs = word_pairs = {'input':[], 'output':[]}\n",
    "    word_pairs = dataset['train'][np.random.choice(len(dataset['train']), n_icl_examples, replace=False)]\n",
    "\n",
    "    word_pairs_test = dataset['test'][n]\n",
    "\n",
    "    prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=False, prefixes=prefixes, separators=separators)\n",
    "    zs_prompt_data = word_pairs_to_prompt_data(zs_word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=False, shuffle_labels=True, prefixes=prefixes, separators=separators)\n",
    "\n",
    "    query = prompt_data['query_target']['input']\n",
    "    target = prompt_data['query_target']['output']\n",
    "\n",
    "    source_token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query)\n",
    "    token_labels, zs_prompt_string = get_token_meta_labels(zs_prompt_data, tokenizer, query)\n",
    "\n",
    "    data_pair = preprocess([zs_prompt_string], [target], tokenizer)\n",
    "    data_pair[\"source_input_ids\"] = preprocess([prompt_string], [target], tokenizer)[\"input_ids\"]\n",
    "    \n",
    "    assert source_token_labels[-1][2] == \"query_predictive_token\"\n",
    "    source_predictive_token_idxs = source_token_labels[-1][0]\n",
    "    data_pair[\"source_predictive_token_idxs\"] = source_predictive_token_idxs\n",
    "    \n",
    "    assert token_labels[-1][2] == \"query_predictive_token\"\n",
    "    predictive_token_idxs = token_labels[-1][0]\n",
    "    data_pair[\"predictive_token_idxs\"] = predictive_token_idxs\n",
    "    \n",
    "    zs_das_eval_set.append(data_pair)\n",
    "    \n",
    "\n",
    "das_train_set = Dataset.from_list(das_train_set)\n",
    "das_eval_set = Dataset.from_list(das_eval_set)\n",
    "zs_das_eval_set = Dataset.from_list(zs_das_eval_set)\n",
    "das_train_set.set_format(type='torch', columns=['input_ids', 'labels', 'source_input_ids', 'source_predictive_token_idxs', 'predictive_token_idxs'])\n",
    "train_dataloader = DataLoader(das_train_set, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "das_eval_set.set_format(type='torch', columns=['input_ids', 'labels', 'source_input_ids', 'source_predictive_token_idxs', 'predictive_token_idxs'])\n",
    "eval_dataloader = DataLoader(das_eval_set, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "zs_das_eval_set.set_format(type='torch', columns=['input_ids', 'labels', 'source_input_ids', 'source_predictive_token_idxs', 'predictive_token_idxs'])\n",
    "zs_eval_dataloader = DataLoader(zs_das_eval_set, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_boundless_das_position_config(model_type, intervention_type, layer):\n",
    "    alignable_config = AlignableConfig(\n",
    "        alignable_model_type=model_type,\n",
    "        alignable_representations=[\n",
    "            AlignableRepresentationConfig(\n",
    "                layer,             # layer\n",
    "                intervention_type, # intervention type\n",
    "                \"pos\",             # intervention unit\n",
    "                1                  # max number of unit\n",
    "            ),\n",
    "        ],\n",
    "        alignable_interventions_type=BoundlessRotatedSpaceIntervention,\n",
    "    )\n",
    "    return alignable_config\n",
    "\n",
    "alignable_config = simple_boundless_das_position_config(type(llama), \"block_output\", 15)\n",
    "alignable = AlignableModel(alignable_config, llama)\n",
    "alignable.set_device(\"cuda\")\n",
    "alignable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = alignable.interventions['layer.15.repr.block_output.unit.pos.nunit.1#0'][0].state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in params.items():\n",
    "    params[k] = v.to(\"cpu\")\n",
    "\n",
    "torch.save(params, \"llama_7b_das.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "t_total = int(len(das_train_set) * epochs)\n",
    "warm_up_steps = 0.1 * t_total\n",
    "optimizer_params = []\n",
    "\n",
    "for k, v in alignable.interventions.items():\n",
    "    optimizer_params += [{'params': v[0].rotate_layer.parameters()}]\n",
    "    optimizer_params += [{'params': v[0].intervention_boundaries, 'lr': 1e-2}]\n",
    "    \n",
    "optimizer = torch.optim.Adam(\n",
    "    optimizer_params,\n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warm_up_steps,\n",
    "    num_training_steps=t_total\n",
    ")\n",
    "\n",
    "# You can define your custom compute_metrics function.\n",
    "def compute_metrics(eval_preds, eval_labels):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for eval_pred, eval_label in zip(eval_preds, eval_labels):\n",
    "        \n",
    "        for i in range(eval_label.shape[0]):\n",
    "            label_idxs = eval_label[i].ne(IGNORE_INDEX).nonzero().squeeze(-1)\n",
    "            # label_idxs = label_idxs[1: ]\n",
    "            left_shifted_idxs = label_idxs\n",
    "            \n",
    "            actual_test_labels = eval_label[i][label_idxs].tolist()\n",
    "            pred_test_labels = [eval_pred[i][idx].argmax(dim=-1) for idx in left_shifted_idxs]\n",
    "            \n",
    "            correct = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += 1\n",
    "            if correct:\n",
    "                correct_count += 1\n",
    "    accuracy = round(correct_count/total_count, 2)\n",
    "    return {\"accuracy\" : accuracy}\n",
    "\n",
    "gradient_accumulation_steps = 4\n",
    "total_step = 0\n",
    "target_total_step = len(das_train_set) * epochs\n",
    "temperature_start = 50.0\n",
    "temperature_end = 0.1\n",
    "temperature_schedule = torch.linspace(\n",
    "    temperature_start, temperature_end, target_total_step\n",
    ").to(torch.bfloat16).to(\"cuda\")\n",
    "alignable.set_temperature(temperature_schedule[total_step])\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    shift_logits = logits[..., :, :].contiguous()\n",
    "    shift_labels = labels[..., :].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    shift_logits = shift_logits.view(-1, alignable.model_config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "    \n",
    "    for k, v in alignable.interventions.items():\n",
    "        boundary_loss = 1. * v[0].intervention_boundaries.sum()\n",
    "    loss += boundary_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  16777218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 16/16 [03:02<00:00, 11.41s/it, loss=2.72, acc=0.5] \n",
      "Epoch: 1: 100%|██████████| 16/16 [03:03<00:00, 11.46s/it, loss=2.66, acc=0.5] \n",
      "Epoch: 2: 100%|██████████| 16/16 [03:04<00:00, 11.50s/it, loss=2.56, acc=0.53]\n",
      "Epoch: 3: 100%|██████████| 16/16 [03:03<00:00, 11.49s/it, loss=2.42, acc=0.53]\n",
      "Epoch: 4: 100%|██████████| 16/16 [03:03<00:00, 11.50s/it, loss=2.26, acc=0.53]\n",
      "Epoch: 5: 100%|██████████| 16/16 [03:04<00:00, 11.52s/it, loss=2.08, acc=0.56]\n",
      "Epoch: 6: 100%|██████████| 16/16 [03:04<00:00, 11.54s/it, loss=1.92, acc=0.56]\n",
      "Epoch: 7: 100%|██████████| 16/16 [03:04<00:00, 11.53s/it, loss=1.78, acc=0.59]\n",
      "Epoch: 8: 100%|██████████| 16/16 [03:05<00:00, 11.57s/it, loss=1.66, acc=0.59]\n",
      "Epoch: 9: 100%|██████████| 16/16 [03:05<00:00, 11.58s/it, loss=1.55, acc=0.62]\n",
      "Epoch: 10: 100%|██████████| 16/16 [03:05<00:00, 11.59s/it, loss=1.43, acc=0.66]\n",
      "Epoch: 11: 100%|██████████| 16/16 [03:05<00:00, 11.60s/it, loss=1.33, acc=0.69]\n",
      "Epoch: 12: 100%|██████████| 16/16 [03:06<00:00, 11.65s/it, loss=1.22, acc=0.75]\n",
      "Epoch: 13: 100%|██████████| 16/16 [03:06<00:00, 11.63s/it, loss=1.13, acc=0.78]\n",
      "Epoch: 14: 100%|██████████| 16/16 [03:06<00:00, 11.64s/it, loss=1.04, acc=0.84]\n",
      "Epoch: 15: 100%|██████████| 16/16 [03:06<00:00, 11.65s/it, loss=0.94, acc=0.88]\n",
      "Epoch: 16: 100%|██████████| 16/16 [03:06<00:00, 11.67s/it, loss=0.86, acc=0.94]\n",
      "Epoch: 17: 100%|██████████| 16/16 [03:07<00:00, 11.69s/it, loss=0.8, acc=0.94] \n",
      "Epoch: 18: 100%|██████████| 16/16 [03:07<00:00, 11.69s/it, loss=0.76, acc=0.94]\n",
      "Epoch: 19: 100%|██████████| 16/16 [03:07<00:00, 11.70s/it, loss=0.73, acc=0.94]\n",
      "Epoch: 20: 100%|██████████| 16/16 [03:07<00:00, 11.73s/it, loss=0.7, acc=0.94] \n",
      "Epoch: 21: 100%|██████████| 16/16 [03:07<00:00, 11.74s/it, loss=0.68, acc=0.94]\n",
      "Epoch: 22: 100%|██████████| 16/16 [03:07<00:00, 11.75s/it, loss=0.66, acc=0.94]\n",
      "Epoch: 23: 100%|██████████| 16/16 [03:08<00:00, 11.76s/it, loss=0.64, acc=0.94]\n",
      "Epoch: 24: 100%|██████████| 16/16 [03:08<00:00, 11.76s/it, loss=0.62, acc=0.94]\n",
      "Epoch: 100%|██████████| 25/25 [1:17:25<00:00, 185.82s/it]\n"
     ]
    }
   ],
   "source": [
    "alignable.model.train() # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(alignable.model))\n",
    "print(\"intervention trainable parameters: \", alignable.count_parameters())\n",
    "\n",
    "train_iterator = trange(\n",
    "    0, int(epochs), desc=\"Epoch\"\n",
    ")\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        \n",
    "        source2base = ([[[idx] for idx in inputs[\"source_predictive_token_idxs\"].tolist()]], [[[idx] for idx in inputs[\"predictive_token_idxs\"].tolist()]])\n",
    "        \n",
    "        _, counterfactual_outputs = alignable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"], \"attention_mask\": inputs[\"source_attention_mask\"]}],\n",
    "            {\"sources->base\": source2base}\n",
    "        )\n",
    "        eval_metrics = compute_metrics(\n",
    "            [counterfactual_outputs.logits], [inputs['labels']]\n",
    "        )\n",
    "        \n",
    "        # loss and backprop\n",
    "        loss = calculate_loss(\n",
    "            counterfactual_outputs.logits, inputs[\"labels\"]\n",
    "        )\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({'loss': loss_str, 'acc': eval_metrics[\"accuracy\"]})\n",
    "        \n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                alignable.set_zero_grad()\n",
    "                alignable.set_temperature(temperature_schedule[total_step])\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 16/16 [02:17<00:00,  8.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot DAS Eval IIA:\n",
      "{'accuracy': 0.59}\n"
     ]
    }
   ],
   "source": [
    "# evaluation on the test set\n",
    "eval_labels = []\n",
    "eval_preds = []\n",
    "with torch.no_grad():\n",
    "    epoch_iterator = tqdm(eval_dataloader, desc=f\"Test\")\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        source2base = ([[[idx] for idx in inputs[\"source_predictive_token_idxs\"].tolist()]], [[[idx] for idx in inputs[\"predictive_token_idxs\"].tolist()]])\n",
    "        \n",
    "        _, counterfactual_outputs = alignable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"], \"attention_mask\": inputs[\"source_attention_mask\"]}],\n",
    "            {\"sources->base\": source2base}\n",
    "        )\n",
    "        \n",
    "        eval_labels += [inputs['labels']]\n",
    "        eval_preds += [counterfactual_outputs.logits]\n",
    "eval_metrics = compute_metrics(eval_preds, eval_labels)\n",
    "print(\"Few-shot DAS Eval IIA:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:45<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot Original Eval Accuracy:\n",
      "0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(tqdm(eval_dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama.device)\n",
    "                    \n",
    "        # aligning forward!\n",
    "        outputs = llama(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            labels=inputs['labels'],\n",
    "            attention_mask=inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "            label_idxs = inputs['labels'][i].ne(IGNORE_INDEX).nonzero().squeeze(-1)\n",
    "            # label_idxs = label_idxs[1: ]\n",
    "            left_shifted_idxs = label_idxs\n",
    "            \n",
    "            actual_test_labels = inputs['labels'][i][label_idxs].tolist()\n",
    "            pred_test_labels = [outputs.logits[i][idx].argmax(dim=-1) for idx in left_shifted_idxs]\n",
    "            \n",
    "            correct = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += 1\n",
    "            if correct:\n",
    "                correct_count += 1\n",
    "                \n",
    "current_acc = round(correct_count/total_count, 2)\n",
    "print(\"Few-shot Original Eval Accuracy:\")\n",
    "print(current_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 16/16 [00:59<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot DAS Eval IIA:\n",
      "{'accuracy': 0.51}\n"
     ]
    }
   ],
   "source": [
    "# evaluation on the test set\n",
    "eval_labels = []\n",
    "eval_preds = []\n",
    "with torch.no_grad():\n",
    "    epoch_iterator = tqdm(zs_eval_dataloader, desc=f\"Test\")\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        source2base = ([[[idx] for idx in inputs[\"source_predictive_token_idxs\"].tolist()]], [[[idx] for idx in inputs[\"predictive_token_idxs\"].tolist()]])\n",
    "        \n",
    "        _, counterfactual_outputs = alignable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"], \"attention_mask\": inputs[\"source_attention_mask\"]}],\n",
    "            {\"sources->base\": source2base}\n",
    "        )\n",
    "        \n",
    "        eval_labels += [inputs['labels']]\n",
    "        eval_preds += [counterfactual_outputs.logits]\n",
    "eval_metrics = compute_metrics(eval_preds, eval_labels)\n",
    "print(\"Zero-shot DAS Eval IIA:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Original Eval Accuracy:\n",
      "0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(tqdm(zs_eval_dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama.device)\n",
    "                    \n",
    "        # aligning forward!\n",
    "        outputs = llama(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            labels=inputs['labels'],\n",
    "            attention_mask=inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "            label_idxs = inputs['labels'][i].ne(IGNORE_INDEX).nonzero().squeeze(-1)\n",
    "            # label_idxs = label_idxs[1: ]\n",
    "            left_shifted_idxs = label_idxs\n",
    "            \n",
    "            actual_test_labels = inputs['labels'][i][label_idxs].tolist()\n",
    "            pred_test_labels = [outputs.logits[i][idx].argmax(dim=-1) for idx in left_shifted_idxs]\n",
    "            \n",
    "            correct = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += 1\n",
    "            if correct:\n",
    "                correct_count += 1\n",
    "                \n",
    "current_acc = round(correct_count/total_count, 2)\n",
    "print(\"Zero-shot Original Eval Accuracy:\")\n",
    "print(current_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundlessRotatedSpaceIntervention(\n",
       "  (rotate_layer): ParametrizedRotateLayer(\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _Orthogonal()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignable.interventions['layer.15.repr.block_output.unit.pos.nunit.1#0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0316,  0.0055,  0.0185,  ...,  0.0017,  0.0020, -0.0249],\n",
      "        [ 0.0302, -0.0058,  0.0077,  ...,  0.0152, -0.0175,  0.0051],\n",
      "        [ 0.0027,  0.0031,  0.0019,  ...,  0.0238,  0.0002,  0.0016],\n",
      "        ...,\n",
      "        [ 0.0065, -0.0095,  0.0061,  ...,  0.0163, -0.0020,  0.0098],\n",
      "        [ 0.0271, -0.0179, -0.0244,  ...,  0.0064, -0.0197,  0.0058],\n",
      "        [-0.0135,  0.0107,  0.0026,  ..., -0.0024, -0.0025,  0.0194]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(alignable.interventions[\"layer.15.repr.block_output.unit.pos.nunit.1#0\"][0].rotate_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
