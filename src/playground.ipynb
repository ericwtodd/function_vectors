{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch, numpy as np\n",
    "import argparse\n",
    "\n",
    "# Include prompt creation helper functions\n",
    "from utils.prompt_utils import *\n",
    "from utils.intervention_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.extract_utils import *\n",
    "from compute_indirect_effect import compute_indirect_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"antonym\"\n",
    "model_name = \"../flan-llama-7b/\"\n",
    "root_data_dir = \"../dataset_files/\"\n",
    "save_path_root = \"../debug\"\n",
    "ie_path_root = \"../results/INS/CIE/flan-llama-7b/Prompt1/antonym\"\n",
    "seed = 42\n",
    "device = \"cuda\"\n",
    "mean_activations_path = \"../results/INS/CIE/flan-llama-7b/Prompt1/antonym/antonym_mean_head_activations.pt\"\n",
    "indirect_effect_path = \"../results/INS/CIE/flan-llama-7b/Prompt1/antonym/antonym_indirect_effect.pt\"\n",
    "n_top_heads = 10\n",
    "eval_edit_layer = -1\n",
    "\n",
    "test_split = float(0.3)\n",
    "n_shots = 0\n",
    "n_trials = 25\n",
    "\n",
    "prefixes = {\"input\": \"Input: \", \"output\": \"Output:\", \"instructions\": \"Instruction: Output the antonym of the given word.\"}\n",
    "separators = {\"input\": \"\\n\", \"output\": \"\\n\", \"instructions\": \"\\n\"}\n",
    "no_instruction = True\n",
    "\n",
    "compute_baseline = True\n",
    "\n",
    "generate_str = False\n",
    "metric = \"exact_match_score\"\n",
    "universal_set = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "Loading:  ../flan-llama-7b/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f7da879c4541f2b41fab149c3f7f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "print(\"Loading Model\")\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n"
     ]
    }
   ],
   "source": [
    "if eval_edit_layer == -1: # sweep over all layers if edit_layer=-1\n",
    "    eval_edit_layer = [0, model_config['n_layers']]\n",
    "\n",
    " # Load the dataset\n",
    "print(\"Loading Dataset\")\n",
    "set_seed(seed)\n",
    "dataset = load_dataset(dataset_name, root_data_dir=root_data_dir, test_size=test_split, seed=seed)\n",
    "\n",
    "if not os.path.exists(save_path_root):\n",
    "    os.makedirs(save_path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:06<00:00, 34.94it/s]\n",
      "100%|██████████| 504/504 [00:14<00:00, 35.61it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed+42)\n",
    "fs_results_validation = n_shot_eval_no_intervention(dataset=dataset, n_shots=n_shots, model=model, model_config=model_config, tokenizer=tokenizer, prefixes=prefixes, separators=separators, compute_ppl=True, test_split='valid')\n",
    "filter_set_validation = np.where(np.array(fs_results_validation['clean_rank_list']) == 0)[0]\n",
    "set_seed(seed)\n",
    "fs_results = n_shot_eval_no_intervention(dataset=dataset, n_shots=n_shots, model=model, model_config=model_config, tokenizer=tokenizer, prefixes=prefixes, separators=separators, compute_ppl=True)\n",
    "filter_set = np.where(np.array(fs_results['clean_rank_list']) == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if no_instruction:\n",
    "    prefixes[\"instructions\"] = \"\"\n",
    "    separators[\"instructions\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations = torch.load(mean_activations_path)\n",
    "indirect_effect = torch.load(indirect_effect_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
