{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /data/public_models/llama/llama_hf_weights/llama-7b/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf688dedfa94cb5a8698c2afd622a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = '../flan-llama-7b/'\n",
    "model_name = '/data/public_models/llama/llama_hf_weights/llama-7b/'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name)\n",
    "EDIT_LAYER = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Compute task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('antonym', seed=0)\n",
    "mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'top_heads' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m FV, top_heads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_universal_function_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_top_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/function_vectors/notebooks/../src/utils/extract_utils.py:451\u001b[0m, in \u001b[0;36mcompute_universal_function_vector\u001b[0;34m(mean_activations, model, model_config, n_top_heads)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-neox\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    440\u001b[0m     top_heads \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m0.0293\u001b[39m), (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0.0224\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m0.019\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m57\u001b[39m, \u001b[38;5;241m0.0079\u001b[39m), (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m43\u001b[39m, \u001b[38;5;241m0.0073\u001b[39m), (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m0.0069\u001b[39m), (\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m0.0065\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m0.0057\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m0.0054\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0.0052\u001b[39m),\n\u001b[1;32m    441\u001b[0m                  (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m0.0052\u001b[39m), (\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m0.005\u001b[39m), (\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m0.0048\u001b[39m), (\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m49\u001b[39m, \u001b[38;5;241m0.0048\u001b[39m), (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m0.0047\u001b[39m), (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m0.0045\u001b[39m), (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m59\u001b[39m, \u001b[38;5;241m0.0043\u001b[39m), (\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m0.0039\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m0.0038\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m33\u001b[39m, \u001b[38;5;241m0.0038\u001b[39m),\n\u001b[1;32m    442\u001b[0m                  (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0.0036\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m0.0035\u001b[39m), (\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m63\u001b[39m, \u001b[38;5;241m0.0032\u001b[39m), (\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m0.0032\u001b[39m), (\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m0.003\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m0.0029\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m0.0029\u001b[39m), (\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m0.0027\u001b[39m), (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m0.0026\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m55\u001b[39m, \u001b[38;5;241m0.0024\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m                  (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m0.0008\u001b[39m), (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m0.0008\u001b[39m), (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m0.0008\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.0008\u001b[39m), (\u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m0.0008\u001b[39m), (\u001b[38;5;241m38\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.0008\u001b[39m), (\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m38\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m),\n\u001b[1;32m    449\u001b[0m                  (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m33\u001b[39m, \u001b[38;5;241m0.0007\u001b[39m), (\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m0.0006\u001b[39m), (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m0.0006\u001b[39m), (\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m0.0006\u001b[39m), (\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m0.0006\u001b[39m), (\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m61\u001b[39m, \u001b[38;5;241m0.0006\u001b[39m)]\n\u001b[0;32m--> 451\u001b[0m top_heads \u001b[38;5;241m=\u001b[39m \u001b[43mtop_heads\u001b[49m[:n_top_heads]\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Compute Function Vector as sum of influential heads\u001b[39;00m\n\u001b[1;32m    454\u001b[0m function_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,model_resid_dim))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'top_heads' referenced before assignment"
     ]
    }
   ],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Shuffled ICL Prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: democracy\\n\\nQ: fascism\\nA: compatible\\n\\nQ: incompatible\\nA: health\\n\\nQ: illness\\nA: software\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: increase\\nA:'\n"
     ]
    }
   ],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [('decrease', 0.87502), ('reduce', 0.02835), ('decre', 0.02314), ('increase', 0.00942), ('reduction', 0.00618)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
